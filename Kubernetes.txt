---------------------------------------------------------------------------------------
INSTALLING KIND AND KUBECTL:

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-linux-amd64

chmod +x ./kind

mv ./kind /usr/local/bin/kind  (/usr/bin/kind for centos)
 	
curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"

chmod +x ./kubectl

mv ./kubectl /usr/local/bin/kubectl


# three node (two workers) cluster config
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker

# three node (two workers) cluster config with calico CNI plugin
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
networking:
  disableDefaultCNI: true # disable kindnet
  podSubnet: 192.168.0.0/16 # set to Calico's default subnet


kind create cluster --config config

------------------------------------------------------------------------------------------------

KUBECTL START:

  600  kubectl cluster-info --context kind-kind
  601  kubectl get nodes
  602  docker ps
  603  docker network ls
  604  docker network inspect kind
  605  kubectl get nodes -o wide
  606  alias k=kubectl
  609  k
  612  k get ns
  613  k get pods -n kubesystem
  614  k get pods -n kube-system
  615  k get pods -n kube-system -o wide
  616  history
  617  history --help
-----------------------------------------------------------------------------------------------------

CREATING PODS:

  618  clear
  619  docker images
  622  kubectl run first --image=k8sdemo_web --dry-run=client -o yaml > pod.yaml
  624  alias k=kubectl
  625  k create -f pod.yaml
  626  k get pods -o wide
  627  k get po
  628  kubectl run second --image=manidockermka/pysimple_web --dry-run=client -o yaml > pod2.yaml
  631  k delete po first second
  632  k get pods -o wide
  633  history

----------------------------------------------------------------------------------------------------------

DEPLOYMENT:

  332  k create deploy mydeploy --image=rajendrait99/infinite2:1.0  --dry-run=client -o yaml > deploy.yaml
  333  vi deploy.yaml (add replicas)
  334  k get all
  335  k create -f deploy.yaml
  336  k get all
  337  k get po
  338  k get po -o wide
  339  k delete po mydeploy-555979b4d6-pjjs2 --force
  340  k get po -o wide

---------------------------------------------------------------------------------------------------------

SCALING:

  656  vi deploy.yaml (change no. of replicas)
  657  k
  658  alias k=kubectl
  659  k apply -f deploy.yaml
  660  k get all
  661  k get po
  662  k edit deploy deploy.yaml
  663  k edit deploy mydeploy
  664  k get po
  668  k scale deploy mydeploy --replicas=15

----------------------------------------------------------------------------------------

CHANGING IMAGES FOR PODS AND ROLLING BACK:

  670  k get deployment
  679  k rollout history deploy mydeploy
  682  vi deploy.yaml (change image version)
  683  k apply -f deploy.yaml
  684  k get all
  685  k get po
  686  k rollout history deploy mydeploy
  687  k rollout undo deploy mydeploy
  688  k rollout history deploy mydeploy
  689  k get po
  690  k rollout history deploy mydeploy --revision=3

------------------------------------------------------------------------------------------

AUTOSCALING:

  696  k autoscale deploy mydeploy --min=3 --max=5 --cpu-percent=80
  697  k get hpa


------------------------------------------------------------------------------------------

YAML:

- name: Meowsy
  species: cat
  foods:
    likes:
    - tuna
    - catnip
    dislikes:
    - ham
    - zucchini
- name: Barky
  species: dog
  foods:
    likes:
    - bones
    - carrots
    dislikes:
    - tuna
- name: Purrpaws
  species: cat
  foods:
    likes:
    - mice
    dislikes:
    - cookies

FOR JSON:

[
  {
    "name": "Meowsy",
    "species" : "cat",
    "foods": {
      "likes": ["tuna", "catnip"],
      "dislikes": ["ham", "zucchini"]
    }
  },
  {
    "name": "Barky",
    "species" : "dog",
    "foods": {
      "likes": ["bones", "carrots"],
      "dislikes": ["tuna"]
    }
  },
  {
    "name": "Purrpaws",
    "species" : "cat",
    "foods": {
      "likes": ["mice"],
      "dislikes": ["cookies"]
    }
  }
]

------------------------------------------------------------------------------------------

GETTING YAML ATRRIBUTES


  726  k explain pods.spec
  727  k explain pods
  728  k explain deploy
  729  k explain service
  730  k explain service.spec


------------------------------------------------------------------------------------------

PODS AND SERVICES IN ACTION:

clone: https://github.com/rskTech/serviceDemo


  782  kubectl create -f db-pod.yml
  783  kubectl create -f db-svc.yml
  786  kubectl create -f web-pod.yaml
  788  kubectl create -f web-svc.yml
  789  k get po
  790  k get no
  791  k get svc
  792  k describe svc web
  793  export NODE_IP=32394
  794  k get no -o wide
  795  export NODE_IP=172.21.0.2
  796  export NODE_PORT=32394
  797  curl http://$NODE_IP:$NODE_PORT/init
  798  curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "1", "user":"John Doe"}' http://$NODE_IP:$NODE_PORT/users/add
  799  curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "2", "user":"Jane Doe"}' http://$NODE_IP:$NODE_PORT/users/add
  800  curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "3", "user":"Bill Collins"}' http://$NODE_IP:$NODE_PORT/users/add
  801  curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "4", "user":"Mike Taylor"}' http://$NODE_IP:$NODE_PORT/users/add
  802  curl http://$NODE_IP:$NODE_PORT/users/1
  803  curl http://$NODE_IP:$NODE_PORT/users/1

db-svc YAML:

apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    name: mysql
    app: demo
spec:
  ports:
  - port: 3306
    name: mysql
    targetPort: 3306
  selector:
    name: mysql
    app: demo


web-svc YAML:

apiVersion: v1
kind: Service
metadata:
  name: web
  labels:
    name: web
    app: demo
spec:
  selector:
    name: web
  type: NodePort
  ports:
   - port: 80
     name: http
     targetPort: 5000
     protocol: TCP


-----------------------------------------------------------------------------------------------------------

DAEMONSET:


  828  vi daemonset.yaml
  829  k apply -f daemonset.yaml
  830  k get pods
  831  history

DAEMONSET YAML:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/

----------------------------------------------------------------------------------------------------------

JOBS:

  833  k create job myjob --image=busybox --dry-run -o yaml -- sleep 20 > job.yaml
  834  k apply -f job.yaml
  835  k explain job.spec
  836  k delete job myjob
  840  vi job.yaml  (add completions and paralellism)
  841  k apply -f job.yaml

---------------------------------------------------------------------------------------------------------

CRONJOBS:


  873  k create cj  mycj --image=busybox --schedule="*/1 * * * *" --dry-run -o yaml -- sleep 10 > cj.yaml
  874  k apply -f cj.yaml
  875  watch kubectl get all

---------------------------------------------------------------------------------------------------------

CREATING CONFIG MAPS:

  889  k create cm mycm --from-literal=host=localhost --from-literal=pass=admin
  890  vi myconfig.conf
  891  k create cm mycm1 --from-file=myconfig.conf
  892  k get cm


---------------------------------------------------------------------------------------------------------

YAML FILE FOR INCLUDING ENV VARIABLES FROM CM:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: po
  name: po
spec:
  containers:
  - image: nginx
    name: po
    resources: {}
    env:
    - name: Host
      valueFrom:
        configMapKeyRef:
          name: myconfig
          key: hostPort
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


-------------------------------------------------------------------------------------------------------

YAML FILE FOR INCLUDING CONFIG FILE INSIDE POD USING VOLUMES:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  volumes:
    - name: myvol
      configMap:
         name: mycon
  containers:
  - image: nginx
    name: nginx
    resources: {}
    env:
     - name: HOST
       valueFrom:
        configMapKeyRef:
          name: mycm1
          key: host
     - name: PASSWORD
       valueFrom:
         configMapKeyRef:
           name: mycm1
           key: pass
    volumeMounts:
     - name: myvol
       mountPath: /etc/lil
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

------------------------------------------------------------------------------------------------------

SECRETS:

  939  k create secret generic mysecret --from-literal=pass=admin
  940  k get secret
  941  vi pod3.yaml  (instead of configMapKeyRef like in previous case, give secretKeyRef)
  942  k apply -f pod3.yaml
  943  k get po
  944  k exec -it po bash
  (Type env in bash to see all env variables)

------------------------------------------------------------------------------------------------------

SERVICE ACCOUNT:

  954  k create sa myuser
  955  k get sa
  957  k get secret

------------------------------------------------------------------------------------------------------

DIFFERENT WAYS POD IS CREATED:

1. pod directly
2. pod from job
3. pod from cronjob
4. pod from daemonset
5. pod from deployment
6. pod from statefulset

-----------------------------------------------------------------------------------------------------

CORDON AND DRAIN:

  992  k cordon kind-worker
  993  k get no
  995  k drain kind-worker2  --ignore-daemonsets
  996  k get no
  997  k uncordon kind-worker kind-worker2
  998  k get no

------------------------------------------------------------------------------------------------------

NODESELECTOR AND SCHEDULING:

 1000  k get no --show-labels
 1001  k label no kind-worker2 hdd=ssd
 1002  k label no kind-worker2 hdd-
 1003  k label no kind-worker2 hdd=ssd
 1004  k run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
 1007  vi pod.yaml  (nodeSelector:
                         hdd:ssd)
 1008  k apply -f pod.yaml
 1009  k delete po nginx
 1010  k cordon kind-worker2
 1011  k apply -f pod.yaml  
 1012  k get po  (In pending state)
 1013  k describe po nginx
 1014  k uncordon kind-worker2
 1015  k get po  (In running state)

-----------------------------------------------------------------------------------------------------

AFFINITY YAML:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: po
  name: po
spec:
  affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                            - matchExpressions:
                                    - key: hdd
                                      operator: In
                                      values:
                                      - ssd

-----------------------------------------------------------------------------------------------------



EX:

k label no kind-worker2 nodeName=nginxNode
k run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
vi pod.yaml
k apply -f pod.yaml

YAML:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  nodeSelector:
    nodeName: nginxNode
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


------------------------------------------------------------------------------------------------------

RESOURCES:

 1033  k run nginx --image=nginx --requests="cpu=0.1m,memory=100M" --limits="cpu=0.2m,memory=200M" --dry-run=client -o yaml > pod6.yaml
 1034  vi pod6.yaml
 1038  k apply -f pod6.yaml
 1040  k describe po nginx

------------------------------------------------------------------------------------------------------

TAINT AND TOLERATIONS:

 1215  k taint no kind-worker hdd=ssd:NoSchedule
 1216  k cordon kind-worker2
 1221  k run po --image=nginx --dry-run=client -o yaml >pod.yaml
 1222  k apply -f pod.yaml
 1223  k get po
 1224  vi pod.yaml
 1225  k explain po.spec.tolerations
 1228  vi pod.yaml
 1229  k apply -f pod.yaml
 1234  k uncordon kind-worker2

YAML:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: po
  name: po
spec:
  tolerations:
  - key: hdd
    operator: Equal
    value: ssd
    effect: NoSchedule
  containers:
  - image: nginx
    name: po
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

--------------------------------------------------------------------------------------------------------------------------

PERSISTENT VOLUMES, PERSISTENT VOLUME CLAIMS, MOUNTING PVC ON PODS:

 1242  vi pv.yaml
 1243  k apply -f pv.yaml
 1244  vi pcv.yaml
 1245  k apply -f pvc.yaml
 1246  k apply -f pcv.yaml
 1247  vi pod7.yaml
 1248  k apply -f pod7.yaml
 1251  k describe po nginx

PV YAML:

apiVersion: v1
kind: PersistentVolume
metadata:
    name: mypv
spec:
    storageClassName: normal
    accessModes:
           - ReadWriteMany
    capacity:
            storage: 2G
    hostPath:
            path: /opt


PVC YAML:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
        name: mypvc
spec:
        storageClassName: normal
        accessModes:
                - ReadWriteMany
        resources:
                requests:
                        storage: 2G


POD YAML:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
    volumeMounts:
            - name: myvol
              mountPath: /etc/lala
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
     - name: myvol
       persistentVolumeClaim:
               claimName: mypvc
status: {}

--------------------------------------------------------------------------------------------------------------------------------

emptyDir VOLUMES: lifetime same as that of the pod  (ephermal volumes are also like emptyDir volumes)

YAML:

apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - image: redis
    name: redis
    volumeMounts:
    - mountPath: /data/redis
      name: my-volume
  volumes:
  - name: my-volume
    emptyDir: {}

------------------------------------------------------------------------------------------------------------------------------



DYNAMIC VOLUME PROVISIONING:

 1274  vi sc.yaml
 1275  k apply -f sc.yaml
 1276  k apply -f pvc.yaml
 1277  k apply -f pv.yaml
 1278  k apply -f pod.yaml

SC YAML:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner  (specify the provisioner here)
volumeBindingMode: WaitForFirstConsumer


---------------------------------------------------------------------------------------------------------------------------------

STATEFULSET:

 1308  k apply -f sfs.yaml
 1310  k get sts
 1311  k apply -f sc.yaml
 1313  k apply -f pv.yaml
 1314  k get po
 1315  k apply -f pv1.yaml
 1316  k apply -f pv2.yaml
 1317  k get po
 1318  k scale sts web --replicas=1
 1319  k get po
 1320  k get pv
 1321  k get sts
 1322  k get pvc
 1324  k scale sts web --replicas=3
 1325  k get po


SFS YAML:

apiVersion: v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi


--------------------------------------------------------------------------------------------------------

STATIC PODS:

    3  docker exec -it kind-control-plane bash
    4  ps -ef
    5  ps -ef | grep kubelet
    6  cat /var/lib/kubelet/config.yaml
    7  cd /etc/kubernetes/manifests
    8  ls
    9  vi kube-apiserver.yaml
   10  cat kube-apiserver.yaml
   11  ls
   12  cat /var/lib/kubelet/config.yaml
   13  pwd
   14  ls
   15  exit
   16  cd /etc/kubernetes/manifests/
   17  ls
   18  rm pod.yaml

 1377  docker cp pod.yaml kind-control-plane:/etc/kubernetes/manifests
 1378  k get po
 1379  docker exec -it kind-control-plane bash (remove pod.yaml from /etc/kubernetes/manifests)
 1380  k get po


--------------------------------------------------------------------------------------------------------

MONITORING:

 1409  wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
 1410  vi components.yaml
 1411  k apply -f components.yaml
 1412  k describe sa metrics-server
 1418  k get deployment metrics-server -n kube-system
 1421  k get pods -n kube-system | grep metrics
 1422  k get apiservice v1beta1.metrics.k8s.io -o yaml
 1423  k top nodes
 1425  k top pods -A


COMPONENTS YAML:

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: system:aggregated-metrics-reader
rules:
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    k8s-app: metrics-server
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      hostNetwork: true
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        image: k8s.gcr.io/metrics-server/metrics-server:v0.4.3
        command:
        - /metrics-server
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          periodSeconds: 10
        name: metrics-server
        ports:
        - containerPort: 4443
          name: https
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          periodSeconds: 10
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      volumes:
      - emptyDir: {}
        name: tmp-dir
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  labels:
    k8s-app: metrics-server
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
  version: v1beta1
  versionPriority: 100


-------------------------------------------------------------------------------------------------------------

ROLE ROLEBINDING CLUSTERROLE CLUSTERROLEBINDING:

ROLE YAML:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods", "pods/logs"]
  verbs: ["get", "watch", "list"]

ROLEBINDING YAML:

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: manishsachin.mk7512@gmail.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


CLUSTERROLE YAML:

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
# "namespace" omitted since ClusterRoles are not  namespaced
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]

CLUSTERROLEBINDING YAML:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-secrets
  namespace: myns # This only grants  permissions within the "development" namespace.
subjects:
- kind: User
  name: manishsachin.mk7512@gmail.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io

------------------------------------------------------------------------------------------------------------------------------

NETWORK POLICIES:

 1451  vi np.yaml
 1452  k run nginx1 --image=nginx
 1453  k run nginx2 --image=nginx
 1454  k run nginx3 --image=nginx
 1455  k label po nginx1 role=db
 1456  k label po nginx2 role=frontend
 1464  k apply -f np.yaml
 1465  k exec -it nginx2 bash
 1466  k exec -it nginx3 bash

NP YAML:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend

--------------------------------------------------------------------------------------------------------------

NODE AND CLUSTER LEVEL RESOURCES:

k api-resources


(the ones mentioned as false are cluster level resources)

--------------------------------------------------------------------------------------------------------------

SECURITY CONTEXTS:

POD WITH SECURITY CONTEXT:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: sleep
  name: sleep
spec:
  containers:
  - args:
    - sleep
    - "1000"
    image: busybox
    name: sleep
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

k apply -f pod.yaml

k exec -it sleep sh

   0 ls
   1 ps
   2 ls -l /proc
   3 cd /proc/1
   4 cat status


------------------------------------------------------------------------------------------------

ANNOTATIONS:

 1676  k annotate po sleep description="This pod is created for so and so purposes"
 1677  k annotate po sleep description-

------------------------------------------------------------------------------------------------

INGRESS AND INGRESS CONTROLLER:

 1689  cd k8s_material/
 1690  ls
 1691  cd ingress
 1692  ls
 1693  k apply -f deploy.yaml
 1694  k get po
 1695  k delete po nginx
 1696  kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
 1697  kubectl expose deployment web --type=NodePort --port=8080
 1698  curl 172.18.0.2:32570
 1699  k get no -o wide
 1700  k get all
 1701  curl 172.21.0.3:31903
 1702  kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0
 1703  k get all
 1704  kubectl expose deployment web2 --port=8080 --type=NodePort
 1705  k get all
 1706  curl 172.21.0.3:31836/TCP
 1707  k apply -f ingress.yaml
 1712  vi /etc/hosts
 1715  curl hello-world.info/v1
 1716  curl hello-world.info/v2

---------------------------------------------------------------------------------------------------

HELM:

INSTALL:

curl -LO https://get.helm.sh/helm-v3.4.0-linux-amd64.tar.gz

tar -C /tmp/ -zxvf helm-v3.4.0-linux-amd64.tar.gz

rm helm-v3.4.0-linux-amd64.tar.gz

mv /tmp/linux-amd64/helm /usr/local/bin/helm

chmod +x /usr/local/bin/hel



helm create my-app

cd my-app

rm -rf templates/*

rm values.yaml

cd templates

vi deploy.yaml

DEPLOY YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: {{.Values.myapp.label}}
  name: {{.Values.myapp.label}}
spec:
  replicas: {{.Values.myapp.replicas}}
  selector:
    matchLabels:
      app: {{.Values.myapp.label}}
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: {{.Values.myapp.label}}
    spec:
      containers:
      - image: {{.Values.myapp.container1.image}}
        name: {{.Values.myapp.container1.name}}
        resources: {}
status: {}


vi job.yaml

JOB YAML:

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: {{.Values.job.name}}
spec:
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - command:
        - sleep
        - "20"
        image: {{.Values.job.image}}
        name: {{.Values.job.name}}
        resources: {}
      restartPolicy: Never
status: {}

cd ..

vi values.yaml

VALUES YAML:

myapp:
   label: mydeploy
   replicas: 3
   container1:
           image: rajendrait99/infinite2:1.0
           name: myapp1
   container2:
           image: nginx
           name: myapp2


job:
        name: myjob
        image: busybox
        command1: sleep
        command2: 20


cd .. 

helm install my-app my-app/ --values my-app/values.yaml

helm uninstall my-app

helm upgrade my-app my-app/ --values my-app/values.yaml (If any changes in values.yaml)

-------------------------------------------------------------------------------------------------------------------------------------

k8s_material git link https://github.com/rskTech/k8s_material.git

-------------------------------------------------------------------------------------------------------------------------------------

READINESS AND LIVENESS PROBE:

 1786  vi pod.yaml
 1792  k apply -f pod.yaml
 1793  k describe po sleep
 1794  k delete po sleep
 1795  vi pod.yaml
 1796  k get po
 1797  k apply -f pod.yaml
 1798  k get po
 1799  k describe po sleep

POD YAML FOR READINESS EXEC:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: sleep
  name: sleep
spec:
  containers:
  - args:
    - sleep
    - "1000"
    image: busybox
    name: sleep
    readinessProbe:
            exec:
              command:
              - ls
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

SIMILARLY WE HAVE livenessProbe (give k explain)

--------------------------------------------------------------------------------------------------------------------------




Rajendra mail-id: rajendrait99@gmail.com

























		









